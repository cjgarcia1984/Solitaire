dqn:
  train:
    total_timesteps: 200000000
    log_interval: 40
  model:
    learning_rate: 0.0001
    learning_starts: 100000
    batch_size: 512
    buffer_size: 5000000
    exploration_fraction: 0.5
    exploration_final_eps: 0.05
    target_update_interval: 100000
    gamma: 0.999
    tau: 0.05
    policy_kwargs:
      net_arch: [256, 256]
    train_freq: [10, "episode"]
    #gradient_steps: 5
    #replay_buffer_class: "PrioritizedReplayBuffer"
    #replay_buffer_kwargs: {"alpha": 0.6}
  test:
    episodes: 4
    steps: 11000
  save_interval: 500000
debug: true
cards_per_turn: 1
num_t_stacks: 7
random_seed: 5
show_messages: false
log_path: /mnt/c/solitaire_logs
tb_log_path: "/mnt/c/solitaire_logs/tb_logs"

clear_logs: true

env:
  num: 1
  save_every: 100000
  stagnation_threshold: 100
  check_available_moves: False
  max_steps_per_game: 10000
